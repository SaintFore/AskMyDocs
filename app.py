from dotenv import load_dotenv
import streamlit as st
import backend as be
from langchain_core.messages import AIMessage, HumanMessage

st.set_page_config(page_title="My AI Agent", page_icon="ğŸ¤–")
st.title("ğŸ¤– æœ¬åœ°å…¨èƒ½çŸ¥è¯†åº“åŠ©æ‰‹")

load_dotenv()

if "config" not in st.session_state:
    st.session_state["config"] = be.AppConfig(chunk_size=500, chunk_overlap=50, k=5)

config = st.session_state["config"]

with st.sidebar:
    st.header("âš™ï¸ é…ç½®é¢æ¿")
    chunk_size = st.number_input("Chunk Size", value=500, step=50)
    chunk_overlap = st.number_input("Chunk Overlap", value=50)
    k_val = st.slider("æ£€ç´¢æ•°é‡ (K)", 1, 10, 5)
    if st.button("è®¾ç½®"):
        st.session_state["config"] = be.AppConfig(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, k=k_val
        )
        st.success("é…ç½®å·²ç»æ›´æ–°")


@st.cache_resource
def create_agent(config: be.AppConfig):
    retriever = be.init_vectorstore(config).as_retriever(search_kwargs={"k": config.k})
    tools = be.create_tools(retriever=retriever)
    agent_executor = be.init_agent(tools=tools, config=config)
    return agent_executor


try:
    agent_executor = create_agent(config)
except Exception as e:
    st.error(f"Error {e}")
    st.stop()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "ui_messages" not in st.session_state:
    st.session_state.ui_messages = []


for msg in st.session_state.ui_messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])


if prompt := st.chat_input("è¾“å…¥é—®é¢˜..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.ui_messages.append({"role": "user", "content": prompt})

    # é˜²æ­¢contextçˆ†ç‚¸
    recent_history = st.session_state.chat_history[-10:]
    current_history = recent_history + [HumanMessage(content=prompt)]
    with st.chat_message("ai"):
        # expanded=True è¡¨ç¤ºé»˜è®¤å±•å¼€ï¼Œè®©ç”¨æˆ·çœ‹åˆ° AI åœ¨å¹²æ´»
        status_container = st.status("ğŸ¤– AI æ­£åœ¨æ€è€ƒ...", expanded=True)

        try:
            generated_msgs = []
            full_response = ""

            events = agent_executor.stream({"messages": current_history})

            for event in events:
                # event å­—å…¸çš„ key æ˜¯èŠ‚ç‚¹å (å¦‚ 'llm', 'tools')
                # value æ˜¯è¯¥èŠ‚ç‚¹çš„è¾“å‡º (å¦‚ {'messages': [...]})
                for node_name, values in event.items():
                    if "messages" in values:
                        new_messages = values["messages"]
                        last_msg = new_messages[-1]

                        generated_msgs.append(last_msg)

                        if isinstance(last_msg, AIMessage) and last_msg.tool_calls:
                            tool_name = last_msg.tool_calls[0]["name"]
                            tool_args = last_msg.tool_calls[0]["args"]
                            status_container.write(f"ğŸ› ï¸ **è®¡åˆ’è°ƒç”¨å·¥å…·**: `{tool_name}`")
                            status_container.json(tool_args)

                        elif node_name == "tools":
                            # ä¸ºäº†ä¸è®©å¤§é‡æ–‡æœ¬åˆ·å±ï¼Œæˆªå–å‰200å­—
                            content_preview = last_msg.content[:200]
                            if len(last_msg.content) > 200:
                                content_preview += "..."
                            status_container.write(
                                f"ğŸ“š **å·¥å…·è¿”å›ç»“æœ**: {content_preview}"
                            )

                        elif (
                            isinstance(last_msg, AIMessage) and not last_msg.tool_calls
                        ):
                            full_response = last_msg.content
                            # è¿™é‡Œæš‚æ—¶ä¸æ˜¾ç¤ºï¼Œç­‰å¾ªç¯ç»“æŸåœ¨å¤–é¢ç»Ÿä¸€æ˜¾ç¤ºï¼Œæˆ–è€…ä½ æƒ³åœ¨ status é‡Œä¹Ÿæ˜¾ç¤º

            status_container.update(
                label="âœ… å›ç­”å®Œæˆ", state="complete", expanded=False
            )

            if full_response:
                st.markdown(full_response)
            else:
                # å…œåº•ï¼šä¸‡ä¸€å¾ªç¯é‡Œæ²¡æŠ“åˆ° contentï¼Œå°è¯•ä» generated_msgs æ‰¾æœ€åä¸€æ¡
                if generated_msgs and isinstance(generated_msgs[-1], AIMessage):
                    full_response = generated_msgs[-1].content
                    st.markdown(full_response)

            st.session_state.ui_messages.append(
                {"role": "assistant", "content": full_response}
            )

            # æ›´æ–° LangChain è®°å¿†
            st.session_state.chat_history.append(HumanMessage(content=prompt))
            st.session_state.chat_history.extend(generated_msgs)

        except Exception as e:
            status_container.update(label="âŒ å‘ç”Ÿé”™è¯¯", state="error")
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
