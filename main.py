from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_core.documents.base import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import GoogleGenerativeAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import OllamaLLM
from langchain_core.prompts import (
    MessagesPlaceholder,
    PromptTemplate,
    ChatPromptTemplate,
)
from langchain_community.document_loaders import TextLoader, chatgpt
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.messages import AIMessage, HumanMessage

# from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os

from langchain.agents import tool, create_tool_calling_agent, AgentExecutor


import re

load_dotenv()

# print(docs)
DB_PATH = "./chroma_db"
os.makedirs(DB_PATH, exist_ok=True)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " "],
)
# for i in split_clean[:100]:
#     print(repr(i.page_content))
#     print("*" * 50)
base_url = "http://192.168.31.60:11434"
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
embeddings_ollama = OllamaEmbeddings(base_url=base_url, model="embeddinggemma:300m")
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
llm_ollama = OllamaLLM(base_url=base_url, model="gemma3:4b")

vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings_ollama)
print(vectorstore._collection_metadata)

if vectorstore._collection.count() == 0:
    print("æ•°æ®åº“ä¸ºç©º")
    loader = TextLoader("./books/Learning.txt", encoding="utf8")
    docs = loader.load()

    split = text_splitter.split_documents(documents=docs)
    split_clean = [
        Document(
            page_content=re.sub(r"\s+", " ", chunk.page_content),
            metadata=chunk.metadata,
        )
        for chunk in split
    ]
    split_clean = split_clean
    for single in split_clean[:50]:
        print(single.page_content)
        print("*" * 50)
        print(single.metadata)
        print("-" * 50)
    print(f"æ­£åœ¨å­˜å…¥{len(split_clean)}ä¸ªç‰‡æ®µ")
    vectorstore.add_documents(split_clean)
else:
    print(f"å·²ç»æœ‰æ•°æ®åº“: {vectorstore._collection}")


# texts = ["hello world!", "ä½ å¥½ï¼Œä¸–ç•Œï¼", "cat", "dog"]
#
# vectors = embeddings.embed_documents(texts=texts)

# print(len(vectors[0]))
# print(vectors[0][:10])

# documents = [
#     "è¿™é‡Œçš„æ™šé¤çœŸå¥½åƒ",
#     "ä»Šå¤©å¤©æ°”ä¸é”™",
#     "çŒ«å–œæ¬¢åƒé±¼",
#     "æˆ‘æ˜¯ä¸€åç¨‹åºå‘˜",
#     "The dog is barking",
# ]


print(vectorstore._collection.count())
# results = vectorstore.as_retriever().get_relevant_documents("æµ‹è¯•")
# print(results)

retriever = vectorstore.as_retriever(search_kwargs={"k": 5})


@tool
def search_book(query: str) -> str:
    """åªæœ‰åœ¨éœ€è¦çš„æ—¶å€™æ‰æŸ¥é˜…ä¹¦ç±ï¼Œè¾“å…¥æ˜¯æŸ¥è¯¢çš„é—®é¢˜"""
    docs = retriever.invoke(query)
    return "\n\n".join([d.page_content for d in docs])


@tool
def calculate_multiply(a: int, b: int) -> int:
    """è®¡ç®—ä¸¤ä¸ªæ•°å­—çš„ä¹˜ç§¯"""
    return a * b


chat_history = []
tools = [search_book, calculate_multiply]
agent_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸ä»…èƒ½æŸ¥ä¹¦ç±ï¼Œé‡åˆ°è®¡ç®—é¢˜è¿˜èƒ½ä½¿ç”¨è®¡ç®—å™¨çš„äººå·¥æ™ºèƒ½ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),  # å…³é”®ï¼šç»™ AI ç•™å‡ºæ€è€ƒå’Œè°ƒç”¨å·¥å…·çš„ç©ºé—´
    ]
)
# llm_bind_tools = llm.bind_tools(tools=tools)  # åé¢çš„create_tool_calling_agentä¼šè‡ªåŠ¨ç»‘å®šå·¥å…·çš„

agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# print("ğŸ•µï¸ Agent å¼€å§‹æ‰§è¡Œ...")
# agent_executor.invoke({"input": "åˆ»æ„ç»ƒä¹ éœ€è¦å¤šé•¿æ—¶é—´ï¼ŒæŠŠè¿™ä¸ªæ—¶é—´ä¹˜ä»¥10æ˜¯å¤šå°‘"})
#
# contextualize_q_system_prompt = """
#     ç»™å®šä¸€æ®µèŠå¤©å†å²å’Œç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œ
#     å¦‚æœè¯¥é—®é¢˜å¼•ç”¨äº†å†å²ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œè¯·å°†å…¶é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œä½¿å…¶ä¸éœ€è¦å†å²ä¸Šä¸‹æ–‡ä¹Ÿèƒ½è¢«ç†è§£ã€‚
#     ä¸è¦å›ç­”é—®é¢˜ï¼Œåªéœ€è¿”å›æ”¹å†™åçš„é—®é¢˜ï¼›å¦‚æœæ²¡æœ‰å¿…è¦æ”¹å†™ï¼Œåˆ™åŸæ ·è¿”å›ã€‚
#     """
# contextualize_q_prompt = PromptTemplate.from_template(
#     contextualize_q_system_prompt
#     + "\n\nèŠå¤©å†å²:\n{chat_history}\n\næœ€æ–°é—®é¢˜:\n{input}"
# )
#
# history_retriever = create_history_aware_retriever(
#     llm=llm, retriever=retriever, prompt=contextualize_q_prompt
# )
#
#
# qa_system_prompt = """
#     ä¸Šä¸‹æ–‡ (Context):
#     {context}
#     """
#
# qa_prompt = PromptTemplate.from_template(qa_system_prompt + "\né—®é¢˜: {input}")
# question_answer_chain = create_stuff_documents_chain(llm=llm_ollama, prompt=qa_prompt)
#
# rag_chain = create_retrieval_chain(history_retriever, question_answer_chain)

# # rag_chain = (
# #     {"context": retriever, "question": RunnablePassthrough()}
# #     | template
# #     | llm_ollama
# #     | StrOutputParser()
# # )
#
# # docs = retriever.get_relevant_documents("nothing to do")
# # print(docs)
# # print(len(docs))
#
# # question = "ä»Šå¤©ä¼šä¸‹é›¨ä¹ˆ"
# # print(f"é—®: {question}")
# # answer = rag_chain.invoke(question)
# # print(f"ç­”: {answer}")
# # query = "coding"
# # # results = db.similarity_search(query=query, k=2)
# # print(results)
# #

while True:
    user_input = input("\nHuman: ")
    if user_input.lower() in ["q", "quit", "exit"]:
        print("ä¸‹æ¬¡å†è§")
        break
    if not user_input.strip():
        continue

    print("AIæ­£åœ¨æ€è€ƒ...", end="", flush=True)
    # response = rag_chain.invoke({"input": user_input, "chat_history": chat_history})

    response = agent_executor.invoke(
        {"input": user_input, "chat_history": chat_history}
    )
    # print(response)
    print(f"\rAI: {response['output']}")

    # å†å²
    chat_history.append(HumanMessage(content=user_input))
    chat_history.append(AIMessage(content=response["output"]))

    # source_docs = retriever.invoke(
    #     user_input
    # )  # æ­¤retrieverå¹¶éhistory_retrieverï¼Œè¿™é‡Œæœ‰bug
    # for i in chat_history:
    #     print(i)
    # print(len(response["context"]))
